{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Transformer:\n",
    "\n",
    "Allen's Relations in Start time - Duration Plot as in [1]: \n",
    "\n",
    "![alt text](./Allen'sRelationsAndthe2DRepresentation.png \"Allen's Relations in Start time - Duration Plot\")\n",
    "\n",
    "\n",
    "[1] Incremental Temporal Pattern Mining Using Efficient Batch-Free Stream Clustering, Thomas Seidl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Suche einen Pfad raus!\n",
    "#TODO: Clean real data from daily to parameterwise activities\n",
    "#TODO: Pseudonimize data for uni\n",
    "#TODO: Drifts?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experiments:\n",
    "\n",
    "Experiment logs conain variations of: \n",
    "- Time\n",
    "- Structure\n",
    "- Allen relationships between activities\n",
    "\n",
    "<style> table {display: block} </style>\n",
    "    \n",
    "    \n",
    "| # Traces | # Unique activities | Filename | Time variation | Structure variation | Allen variation  | Further description | Expectations | Observations | Further ideas |\n",
    "|----------|---------------------|:---------|:---------------|---------------------|------------------|---------------------|:-------------|--------------|---------------|\n",
    "|       1  |          7          | catts_1trace_7act_0dur_running-example.csv | Constant 0 sec duration | None | \"after/before\" relations only  || Points are equidistant.<br> S-E plot: Trace is straight over the diagonal and only 2 allen lines can be seen for any activity.<br> S-D plot: Trace horizontal at 0 and only one allen line is visible. |Consistency verified for activities||\n",
    "|       1  |          7          | catts_1trace_7act_12hDur_running-example.csv | Constant 12h duration | None | \"after/before\" relations only  || Points are equidistant. <br> S-E plot: Trace is straight with a constant distance from diagonal. <br> S-D plot: Trace is horizontal.| Allen lines confirm \"after\", \"before\" relations bt. events. | Further investigate average plot |\n",
    "|       1  |          7          | catts_1trace_7act_1dayDur_meets_running-example.csv | Constant 1d duration | None | Contains \"meets\" relations || 'reinitiate request' is met by 'decide' and meets 'examine thoroughly'| Allen lines in both plots are consistent with 'meets' relation ||\n",
    "|       1  |          7          | catts_1trace_7act_meetsStartedByFinishedBy_running-example.csv | Constant duration | None | Contains \"started by\", \"meets\" & \"finished by\" relations || 'reinitiate request' is met by 'check ticket', finished by 'decide', started by 'examine thoroughly' and meets a second 'check ticket' | Allen lines in both plots are consistent with 'meets', 'started by', and 'finished by' relations | |\n",
    "|       1  |          7          | catts_1trace_7act_meetsStartsFinishes_running-example.csv | Constant duration | None | Contains \"starts\", \"meets\" & \"finishes\" relations || 'reinitiate request' is met by 'check ticket', starts 'decide', finishes 'examine thoroughly' and meets a second 'check ticket' | Allen lines in both plots are consistent with 'meets', 'starts', and 'finishes' relations ||\n",
    "|       1  |          7          | catts_1trace_7act_meetsOverlapsOverlappedBy_running-example.csv | Constant duration | None | Contains \"overlaps\", \"meets\" & \"overlapped by\" relations || 'reinitiate request' is met by 'check ticket', overlaps 'decide', is overlapped by 'examine thoroughly' and meets a second 'check ticket' | Allen lines in both plots are consistent with 'meets', 'overlaps', and 'overlapped by' relations || \n",
    "|       1  |          7          | catts_1trace_7act_varDur_running-example.csv | Variable duration bt. activities | None | \"after/before\" relations only || S-E plot: Is not a straight line. S-D plot: Is not a straight line. |Instead of diagonal draw trace horizontal?|         \n",
    "|       2  |          7          | catts_2traces_7act_12h1actDevEndTime_running-example.csv | Constant deviation bt. traces for one activity | None | \"after/before\" relations only | Expectation and reality traces | One trace is a as in 0 duration plots, the other deviates only on one point forming a peak in the trace line. | Peaks can be used to identify outliers regarding time deviations from expectations. ||\n",
    "|       2  |          7          | catts_2traces_7act_12hconstDevEndTime_running-example.csv | Constant deviation bt. traces for all activities | None | \"after/before\" relations only | Expectation and reality traces | Traces are both straight equidistant lines. | Distances from expectations can be used to identify outliers and drifts/shifts? ||\n",
    "|       2  |          7          | catts_2traces_7act_varDevEndTime_running-example.csv | Variable deviation on 'end_time' bt. traces for every activity | None | \"after/before\" relations only | Expectation and reality traces | One trace deviates only on 'end_time' from the other and is not a straight line. | Time outliers can be identified by y distance to corresponding expectation activity point. <br> | Investigate both 'timestamps' deviations |\n",
    "|       2  |          7          | catts_2traces_7act_varDevStartTime_noShift_running-example.csv | Variable deviation on 'start_time' bt. traces for every activity but first one. | None | \"after/before\" relations only | Expectation and reality traces | One trace deviates only on 'start_time' from the other and is not a straight line. <br> S-E plot: Points horizontally of each other <br> S-D plot: No straight line. | S-D plot:  Deviating activities show in the same 'finish' allen line.  | Per activity normalization? Drawing lines between corresponding activity points? |  \n",
    "|       2  |          7          | catts_2traces_7act_varDevStartTime_running-example.csv | Variable deviation on 'start_time' bt. traces for every activity | None | \"after/before\" relations only | Expectation and reality traces | One trace deviates only on 'start_time' from the other and is not a straight line. <br> S-E plot: Points horizontally of each other <br> S-D plot: No straight line. | Time outliers can be identified by y distance to corresponding expectation activity point. <br> S-E plot: If first timestamp is not same in both traces, rest of timestamps are shifted. If activity's normalized 'start_time' is the same between traces, it will show in same vertical. <br> S-D plot: Activities without varation are on same horizontal, since duration stayed the same. If activity's normalized 'start_time' is the same between traces, it will show in same vertical. Other wise the rest of the trace is shifted by the first activity's 'start_time' deviation | Add plot option: plot normalization to only one trace instead of every trace. |\n",
    "|       2  |          7          | catts_2traces_7act_varDevBothTimes_12hDur_noShift_running-example.csv | Variable deviation on 'start_time' and 'end_time', constant 12h duration bt. traces for every activity but first one. | None | \"after/before\" relations only | Expectation and reality traces | One trace deviates on 'start_time' and 'end_time' from the other and is a straight line overlapp, given constant duration bt. traces. <br> S-E plot: Parallel to diagonal. <br> S-D plot: Horizontal at 12h duration | S-D plot:   ||\n",
    "|       2  |          7          | catts_2traces_7act_varDevBothTimes_12hDur_running-example.csv | Variable deviation on 'start_time' and 'end_time', constant 12h duration bt. traces for every activity. | None | \"after/before\" relations only | Expectation and reality traces ||||\n",
    "|       2  |          7          | catts_2traces_7act_varDevBothTimes_6hDurDev_noShift_running-example.csv | Variable deviation on 'start_time' and 'end_time' (all act's but first), constant 6h duration deviation bt. traces for every activity. | None | \"after/before\" relations only | Expectation and reality traces | One trace deviates on 'start_time', 'end_time' and duration from the other and is a parallel. <br> S-E plot: Parallel to diagonal. <br> S-D plot: Parallel horizontal | S-D plot:   ||\n",
    "|       2  |          7          | catts_2traces_7act_varDevBothTimes_6hDurDev_running-example.csv | Variable deviation on 'start_time' and 'end_time', constant 6h duration deviation bt. traces for every activity. | None | \"after/before\" relations only | Expectation and reality traces ||||\n",
    "|       2  |          7          | catts_2traces_7act_varDevBothTimes_varDurDev_noShift_running-example.csv | Variable deviation on 'start_time' and 'end_time', variable duration deviation bt. traces for every activity but first one. | None | \"after/before\" relations only | first 'check ticket' conforms on 'start_time'; <br> 'reinitiate request' conforms on duration; <br> second 'decide' conforms on 'end_time'; <br>'pay compensation' conforms on both timestamps | Trace '3' is a bent line since it irregularly deviates from the trace '0', which straight line.  <br> S-E plot: Conformant activities share allen lines horizontals, verticals or diagonal. | S-E plot: first 'check ticket' points share vertical; <br> 'reinitiate request' points share diagonal; <br> second 'decide' points share vertical; <br>'pay compensation' points are equal. | Through Allen lines conformity between traces is visible.\n",
    "|       2  |          7          | catts_2traces_7act_varDevBothTimes_varDurDev_running-example.csv | Variable deviation on 'start_time' and 'end_time', variable duration deviation bt. traces for every activity. | None | \"after/before\" relations only | first 'check ticket' conforms on 'start_time'; <br> 'reinitiate request' conforms on duration; <br> second 'decide' conforms on 'end_time'; <br>'pay compensation' conforms on both timestamps  | Trace '3' is a bent line since it irregularly deviates from the trace '0', which straight line. Through shift 'start_time' deviations will not be visible. <br> S-E plot: Conformant activities in 'end_time' and duration share allen lines horizontals, verticals or diagonal.| S-E plot: Through shift lack of deviation on first 'check ticket' is not visible; <br> 'reinitiate request' and 'pay compensation' share diagonal; <br> share vertical; <br>'pay compensation' does not deviate on both timestamps Through shift conformity on first 'check ticket' is not visible.| Through first activity shift conformity of start_times after first activity is not visible.; <br> | \n",
    "|       2  |          7          |  | Constant deviation bt. traces for all activities | 2 activities swapped | \"after/before\" relations only | Expectation and reality traces ||| Erkennt man outlier (structure/allen struktur/time)? |\n",
    "|      10  |          8          |  | Variable deviation bt. traces for every activities | None | \"after/before\" relations only | Expectation and reality traces |||\n",
    "|      10  |          8          |  | Variable deviation bt. traces for every activities | multiple activities swapped | \"after/before\" relations only | Expectation and reality traces |||\n",
    "|      10  |          8          |  | Variable deviation bt. traces for every activities | multiple activities swapped | variations of relations | Expectation and reality traces |||\n",
    "|     100  |          8          |  | Variable deviation bt. traces for every activities | None | \"after/before\" relations only | Expectation and reality traces |||\n",
    "|     100  |          8          |  | Variable deviation bt. traces for every activities | multiple activities swapped | \"after/before\" relations only | Expectation and reality traces |||\n",
    "|     100  |          8          |  | Variable deviation bt. traces for every activities | multiple activities swapped | variations of relations | Expectation and reality traces |||\n",
    "...\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excecution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily//param_catts_daily.2019-09-.csv\n",
      "['param_catts_daily.2019-09-.csv']\n",
      "15711\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>activity</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14463</th>\n",
       "      <td>daily.2019-09-01_09-29-01.csv</td>\n",
       "      <td>AllTasks</td>\n",
       "      <td>2019-09-01 09:29:23</td>\n",
       "      <td>2019-09-03 03:43:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14468</th>\n",
       "      <td>daily.2019-09-01_09-29-01.csv</td>\n",
       "      <td>CrawlFeedTask</td>\n",
       "      <td>2019-09-01 09:42:15</td>\n",
       "      <td>2019-09-01 10:01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14467</th>\n",
       "      <td>daily.2019-09-01_09-29-01.csv</td>\n",
       "      <td>ProxySetupTask</td>\n",
       "      <td>2019-09-01 09:42:15</td>\n",
       "      <td>2019-09-01 09:54:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14464</th>\n",
       "      <td>daily.2019-09-01_09-29-01.csv</td>\n",
       "      <td>DumpTask(target_filename=None)(chunk=prep)(sql_filename=daily_urls)</td>\n",
       "      <td>2019-09-01 09:42:15</td>\n",
       "      <td>2019-09-01 09:43:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14465</th>\n",
       "      <td>daily.2019-09-01_09-29-01.csv</td>\n",
       "      <td>DumpTask(target_filename=None)(chunk=prep)(sql_filename=regular_urls)</td>\n",
       "      <td>2019-09-01 09:42:15</td>\n",
       "      <td>2019-09-01 10:09:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                case  \\\n",
       "14463  daily.2019-09-01_09-29-01.csv   \n",
       "14468  daily.2019-09-01_09-29-01.csv   \n",
       "14467  daily.2019-09-01_09-29-01.csv   \n",
       "14464  daily.2019-09-01_09-29-01.csv   \n",
       "14465  daily.2019-09-01_09-29-01.csv   \n",
       "\n",
       "                                                                    activity  \\\n",
       "14463  AllTasks                                                                \n",
       "14468  CrawlFeedTask                                                           \n",
       "14467  ProxySetupTask                                                          \n",
       "14464  DumpTask(target_filename=None)(chunk=prep)(sql_filename=daily_urls)     \n",
       "14465  DumpTask(target_filename=None)(chunk=prep)(sql_filename=regular_urls)   \n",
       "\n",
       "               start_time            end_time  \n",
       "14463 2019-09-01 09:29:23 2019-09-03 03:43:13  \n",
       "14468 2019-09-01 09:42:15 2019-09-01 10:01:16  \n",
       "14467 2019-09-01 09:42:15 2019-09-01 09:54:13  \n",
       "14464 2019-09-01 09:42:15 2019-09-01 09:43:44  \n",
       "14465 2019-09-01 09:42:15 2019-09-01 10:09:02  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_list = []\n",
    "appended = pd.DataFrame()\n",
    "#LUIGI_LOG_PATH='/usr/local/trustyou/home/andream/nfs/processmining/experiments'\n",
    "LUIGI_LOG_PATH = '/usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/'\n",
    "#LUIGI_LOG_PATH = '/usr/local/trustyou/home/andream/nfs/processmining/minilogs/table-precomp/logs/tch/'\n",
    "for filename in os.listdir(LUIGI_LOG_PATH):\n",
    "    #if filename.endswith('.csv') and filename.startswith('catts_2traces_7act_varDevBothTimes_varDurDev_running-example.csv'):\n",
    "    #if filename.endswith('.csv') and filename.startswith('catts_1trace_7act_v'):\n",
    "    #if filename.endswith('.csv') and filename.startswith('catts_1trace_7act_0dur_running-example.csv'):\n",
    "    #if filename.endswith('.csv') and filename.startswith('catts_1trace_7act_1dayDur_meets_running-example.csv'):\n",
    "    #if filename.endswith('.csv') and filename.startswith('catts_1trace_7act_meetsStarts'):\n",
    "    if filename.endswith('.csv') and filename.startswith('p'):\n",
    "        log_path = LUIGI_LOG_PATH+'/'+filename\n",
    "        csv_list.append(filename)\n",
    "        print('Preprocessing... ',log_path)\n",
    "        df = pd.read_csv(log_path, index_col=False)\n",
    "        appended = appended.append(df)\n",
    "\n",
    "#appended_df\n",
    "csv_list.sort()\n",
    "print(csv_list)\n",
    "\n",
    "filename = os.path.splitext(csv_list[0])[0]\n",
    "\n",
    "#appended_df.to_datetime(start_time, format=\"%Y-%m-%d:%H:%M:%S\").sort_values()\n",
    "appended['start_time'] = pd.to_datetime(appended['start_time'], format=\"%Y-%m-%d %H:%M:%S\").sort_values()\n",
    "appended['end_time'] = pd.to_datetime(appended['end_time'], format=\"%Y-%m-%d %H:%M:%S\").sort_values()\n",
    "appended = appended.sort_values(by=['start_time'], ascending=True)\n",
    "appended['case'] = appended.apply(lambda row: str(row['case']), axis=1)\n",
    "print(len(appended))\n",
    "appended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254  activities\n",
      "66  short activity names:\n",
      "['AlertsTask', 'ChunkReviewTimelineTask', 'ChunkTask', 'CleanupStarwoodDataTask', 'ClusterScoreTask', 'ConsolidatedAlertsTask', 'ConvertDedupTask', 'ConvertDumpTask', 'ConvertDumpXmlifyTask', 'ConvertFeedTask', 'CrawlChunkTask', 'CrawlFeedTask', 'CrawlPrivateDataTask', 'CrawlTask', 'DBDumpTask', 'DetectLanguageTask', 'DumpTask', 'ExtractChunkTask', 'ExtractTask', 'FetchInputTask', 'FilterOutResponsesTask', 'GeneratePIITask', 'GroupPreDumpTask', 'GroupTask', 'HAProxySetupTask', 'HAProxyTeardownTask', 'MergeNewData', 'NewPrivateDataTask', 'NewPublicDataTask', 'PartitionConvertDumpTask', 'PartitionConvertDumpXmlifyTask', 'PartitionCrawlInputTask', 'PrepTask', 'PrevChunkNewReviewsOriginChangeTask', 'ProxySetupTask', 'ProxyTeardownTask', 'QCCountTask', 'QCUpdateCrawlDateTask', 'QCUpdateTask', 'QCValidateExtractTask', 'QCValidateExtractXmlifyTask', 'QCValidateJsonifyTask', 'QCValidateTask', 'QualityCheckTask', 'ReportTask', 'ResolveFeedIDsTask', 'ResyncReviewPropertiesTask', 'ReviewDedupTask', 'RootTask', 'ScaleScoreFeedTask', 'SemaTask', 'SourceScoreTask', 'SplitCrawlInputTask', 'StatsCrawlInputTask', 'StatsCrawlTask', 'StripPIITask', 'TokenizeTask', 'UpdateCrawlStartTask', 'UpdateDBTask', 'UpdateDatastoreTask', 'UpdateMgmtResponseTask', 'UpdateReviewPropsTask', 'UpdateTimelineTask', 'UserDemographicsFullUpdateTask', 'UserDemographicsIncUpdateApplyTask', 'UserDemographicsIncUpdateCQLTask'] \n",
      "\n",
      "13  cases\n"
     ]
    }
   ],
   "source": [
    "EXCLUDED_TASKS=['AllTasks']\n",
    "\n",
    "counts = appended.groupby(['activity']).size().reset_index(name='counts').sort_values(by=['counts'], ascending=False)\n",
    "counts = counts.sort_values(by=['counts'], ascending = False)\n",
    "counts.head()\n",
    "\n",
    "#appended = appended.head(100)\n",
    "#print(len(appended))\n",
    "unique_act = appended['activity'].unique().tolist()\n",
    "print(len(unique_act), ' activities')\n",
    "#print(unique_act)\n",
    "\n",
    "short_activities=[]\n",
    "for item in unique_act:\n",
    "    short_name = item.split('(',1)[0]\n",
    "    short_activities.append(short_name)\n",
    "unique_short_activities = list(sorted(set(short_activities)-set(EXCLUDED_TASKS)))\n",
    "print(len(unique_short_activities),' short activity names:')\n",
    "print(unique_short_activities,'\\n')\n",
    "\n",
    "unique_trace = appended['case'].unique().tolist()\n",
    "print(len(unique_trace), ' cases')\n",
    "#print(unique_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_from_label(label, color):\n",
    "    return color\n",
    "    \n",
    "def multiline_text(text, max_in_line):\n",
    "    many_lines = []\n",
    "    result=''\n",
    "    i=0\n",
    "    while len(text)>max_in_line and i==0: \n",
    "        many_lines.append(text[0:max_in_line+1])\n",
    "        text = text[max_in_line+1:]\n",
    "    if len(text): \n",
    "        many_lines.append(text)\n",
    "    for item in many_lines:\n",
    "        result += item + '\\n'\n",
    "    return result\n",
    "\n",
    "def title_from_list(act_selection):\n",
    "    result=''\n",
    "    for act in act_selection:\n",
    "        result+=act+'_'\n",
    "    return result\n",
    "    \n",
    "def plot_newline(p1, p2):\n",
    "    ax = plt.gca()\n",
    "    xmin, xmax = ax.get_xbound()\n",
    "\n",
    "    if(p2[0] == p1[0]):\n",
    "        xmin = xmax = p1[0]\n",
    "        ymin, ymax = ax.get_ybound()\n",
    "    else:\n",
    "        ymax = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmax-p1[0])\n",
    "        ymin = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmin-p1[0])\n",
    "\n",
    "    l = mlines.Line2D([xmin,xmax], [ymin,ymax], color='grey', linestyle='--', linewidth=1)\n",
    "    ax.add_line(l)\n",
    "    return l\n",
    "\n",
    "def draw_traces(data_selection, ax, draw_skylines=None):\n",
    "    colormapt = cm.gist_ncar\n",
    "    trace_colorlist = [colors.rgb2hex(colormapt(i)) for i in np.linspace(0, 0.9, len(unique_trace))]\n",
    "    trace_legend = dict(zip(unique_trace, trace_colorlist))\n",
    "    for j, k in enumerate(data_selection['case'].drop_duplicates()):\n",
    "        current = data_selection[data_selection['case']==k]\n",
    "        skyline = get_skyline_points(current)\n",
    "        l = k\n",
    "        c = trace_legend.get(l)\n",
    "\n",
    "        if draw_skylines: \n",
    "            ax.plot(skyline['num_start'], skyline['num_end'], label='skyline '+k, zorder=0, color=c)\n",
    "        else: \n",
    "            ax.plot(current['num_start'], current['num_end'], label='trace '+k, zorder=0, color=c)\n",
    "            \n",
    "def draw_allen_lines(allen_point, ax, yax, duration_plot=None):\n",
    "            x = allen_point['num_start'].values[0]\n",
    "            y = allen_point['num_end'].values[0]\n",
    "\n",
    "            if duration_plot: \n",
    "                ax.axvline(x, c='grey', linewidth=1, linestyle='--')\n",
    "                ax.axvline(x+y, c='grey', linewidth=1, linestyle='--')\n",
    "                plot_newline([x,y],[x+2*y,-y])\n",
    "                plot_newline([x-y,y],[x+y,-y])\n",
    "            else:\n",
    "                ax.plot([x,x],[x,yax],'k-', c='grey', linewidth=1, linestyle='--')\n",
    "                ax.plot([y,y],[y,yax],'k-', c='grey', linewidth=1, linestyle='--')\n",
    "                ax.plot([0,x],[x,x],'k-', c='grey', linewidth=1, linestyle='--')\n",
    "                ax.plot([0,y],[y,y],'k-', c='grey', linewidth=1, linestyle='--')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_point_transformer(title, data_selection, activity=None, traces=None,  allen_point=None, size=None, duration_plot=None, draw_skylines=None, output_path=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    if size: \n",
    "        fig.set_size_inches(18.5, 18.5)\n",
    "\n",
    "    #colormap = cm.nipy_spectral\n",
    "    #colormap = cm.prism\n",
    "    #colormap = cm.tab20\n",
    "    colormap = cm.hsv\n",
    "    #colormap = cm.gist_rainbow\n",
    "    #colormap = cm.gist_ncar\n",
    "    \n",
    "    unique_act = sorted(data_selection['activity'].unique().tolist())\n",
    "    unique_trace = data_selection['case'].unique().tolist()\n",
    "    \n",
    "    colorlist = [colors.rgb2hex(colormap(i)) for i in np.linspace(0, 0.9, len(unique_act))]\n",
    "    legend = dict(zip(unique_act, colorlist))\n",
    "    colorby = 'activity'\n",
    "    \n",
    "    if activity:\n",
    "        data_selection = data_selection.loc[data_selection['activity']==activity].reset_index()\n",
    "        colorlist = [colors.rgb2hex(colormap(i)) for i in np.linspace(0, 0.9, len(unique_trace))]\n",
    "        legend = dict(zip(unique_trace, colorlist))\n",
    "        colorby = 'case'\n",
    "    elif traces:\n",
    "        data_selection = data_selection.loc[data_selection['case'].isin(traces)].reset_index()\n",
    "    \n",
    "    for i, e in enumerate(data_selection['num_start']):\n",
    "        x = data_selection['num_start'][i]\n",
    "        y = data_selection['num_end'][i]\n",
    "        l = data_selection[colorby][i]\n",
    "        c = legend.get(l)\n",
    "\n",
    "        ax.scatter(x, y, label=l, s=50, linewidth=0.1, c=c)\n",
    "    \n",
    "    yin, yax= ax.get_ylim()\n",
    "    xin, xax= ax.get_xlim()\n",
    "    ax.set_xlim(xmin=0)\n",
    "    ax.set_ylim(ymin=0)\n",
    "    \n",
    "    if not duration_plot: \n",
    "        #Draw diagonal\n",
    "        plot_newline([0,0],[max(xax,yax),max(xax,yax)])\n",
    "        ax.set_ylabel('End time [mins]')\n",
    "    else: \n",
    "        ax.set_ylabel('Duration [seconds]')\n",
    "\n",
    "    if traces:\n",
    "        draw_traces(data_selection, ax, draw_skylines=draw_skylines)\n",
    "            \n",
    "    if not allen_point is None :# Weird if statement because of maybe empty object or dataframe\n",
    "        draw_allen_lines(allen_point, ax, yax, duration_plot=duration_plot)    \n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    ax.set_xlabel('Start time [mins]')\n",
    "\n",
    "    xlocs, labels = plt.xticks()\n",
    "    ylocs, labels = plt.yticks()\n",
    "    plt.xticks(xlocs[1:], get_time_list_from_seconds(xlocs[1:]),rotation='vertical')\n",
    "    plt.yticks(ylocs[1:], get_time_list_from_seconds(ylocs[1:])) \n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    \n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    #plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(multiline_text(title, 175))\n",
    "    \n",
    "    if output_path: \n",
    "        print('Saving in ',output_path)\n",
    "        fig.savefig(output_path,  bbox_inches='tight')\n",
    "        \n",
    "    #plt.show()\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "#plot_point_transformer('Point transformer: Activity \\''+ str(snippet['activity'][6]) + '\\' only', snippet , activity='reject request')\n",
    "#plot_point_transformer('Point transformer: All activities in all traces', snippet)\n",
    "#plot_point_transformer('Point transformer: Trace \\''+ str(snippet['case'][0]) + '\\' only', snippet)\n",
    "#plot_point_transformer('Point transformer: Average trace from all activities', snippet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_points(group):\n",
    "    group['zero_point'] = group['start_time'].min()\n",
    "    return group\n",
    "\n",
    "def get_duration(start_time, end_time):\n",
    "    start = datetime.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n",
    "    duration = abs(end - start)\n",
    "    return duration\n",
    "#get_duration(ex['timestamp'][10],ex['timestamp'][1])\n",
    "\n",
    "def get_time_list_from_seconds(list):\n",
    "    result = []\n",
    "    for item in list:\n",
    "        if item < 0: \n",
    "            result.append('')\n",
    "        else: \n",
    "            result.append(datetime.timedelta(seconds=item))\n",
    "    return result\n",
    "\n",
    "def avg_datetime(series):\n",
    "    averages = (series.sum())/len(series)\n",
    "    #averages = time.strftime('%H:%M:%S', time.gmtime(averages))\n",
    "    return averages\n",
    "\n",
    "def get_average_times(group):\n",
    "    group['average_start'] = time.strftime('%H:%M:%S', time.gmtime(avg_datetime(group['num_start'])))\n",
    "    group['average_end'] = time.strftime('%H:%M:%S', time.gmtime(avg_datetime(group['num_end'])))\n",
    "    group['num_start'] = avg_datetime(group['num_start'])\n",
    "    group['num_end'] = avg_datetime(group['num_end'])\n",
    "    return group\n",
    "\n",
    "def get_data_selection_avgtrace(df):\n",
    "    average_trace = df[['case','activity','rel_start','rel_end','num_start','num_end']].iloc[: , :]\n",
    "    average_trace = average_trace.groupby(['activity'])\n",
    "    average_trace = average_trace.apply(get_average_times)\n",
    "    average_trace = average_trace.drop_duplicates('activity', keep='first').reset_index()\n",
    "    average_trace['case'] = 'Average Case'\n",
    "    average_trace = average_trace[['activity','average_start', 'average_end','num_start','num_end', 'case']].sort_values(by=['num_start'])\n",
    "    return average_trace\n",
    "\n",
    "def get_skyline_points(df):\n",
    "    #skyline = pd.DataFrame(columns=['x','y'])\n",
    "    df.sort_values(by=['num_start'])\n",
    "    max_x = []\n",
    "    max_y = []\n",
    "    for i, e in enumerate(df['num_start']):\n",
    "        maxi = max(df['num_start'][0:i+1].values.tolist())\n",
    "        mayi = max(df['num_end'][0:i+1].values.tolist())\n",
    "        if maxi in df[df['num_end']==mayi]['num_start'].values:\n",
    "            max_x.append(maxi)\n",
    "            max_y.append(mayi)\n",
    "\n",
    "    skyline = pd.DataFrame({'num_start':max_x, 'num_end':max_y})\n",
    "    skyline = skyline.drop_duplicates().reset_index()[['num_start','num_end']]\n",
    "\n",
    "    return skyline\n",
    "\n",
    "#first_case = snippet.loc[snippet['case']==snippet['case'][0]].reset_index()\n",
    "#get_skyline_points(first_case).head()\n",
    "\n",
    "def get_duration(start_time, end_time):\n",
    "    start = datetime.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n",
    "    duration = abs(end - start)\n",
    "    return duration\n",
    "#get_duration(ex['timestamp'][10],ex['timestamp'][1])\n",
    "\n",
    "\n",
    "\n",
    "def get_relative_timestamps(df):\n",
    "    relatived = df.copy()\n",
    "    #print('First timestamp in dataframe ', df['start_time'].min())\n",
    "    #print('Last timestamp in dataframe ',df['end_time'].max())\n",
    "    grouped = df.groupby(['case'])\n",
    "    grouped = grouped.apply(get_zero_points)\n",
    "    #print('Grouped:', len(grouped), 'columns', grouped.columns.tolist())\n",
    "\n",
    "    \n",
    "    relatived = pd.merge(grouped, relatived, on = ['case', 'activity', 'start_time', 'end_time'], how = 'inner') \n",
    "    #print('Merged relatived:', len(relatived), 'columns', relatived.columns.tolist())\n",
    "    \n",
    "    excluding = EXCLUDED_TASKS\n",
    "    relatived['rel_start'] = relatived.apply(lambda row: \n",
    "                                           str(get_duration(str(row['zero_point']),\n",
    "                                                            str(row['start_time']))), axis=1)\n",
    "    relatived['rel_end'] = relatived.apply(lambda row: \n",
    "                                           str(get_duration(str(row['zero_point']),\n",
    "                                                            str(row['end_time']))), axis=1)\n",
    "    \n",
    "    relatived['num_start']= list(pd.to_timedelta(relatived['rel_start'], errors=\"coerce\").dt.total_seconds ())\n",
    "    relatived['num_end']= list(pd.to_timedelta(relatived['rel_end'], errors=\"coerce\").dt.total_seconds ())\n",
    "\n",
    "    #relatived = relatived[['case', 'activity','rel_start', 'rel_end', 'num_start', 'num_end', 'start_time', 'end_time']]\n",
    "\n",
    "    relatived = relatived.sort_values(by=['num_start'], ascending=True)\n",
    "    relatived = relatived[~relatived['activity'].isin(excluding)].reset_index()\n",
    "    #print('Exclusive: ', len(relatived), 'columns', relatived.columns.tolist())\n",
    "\n",
    "    \n",
    "    relatived = relatived[['case','activity','rel_start','rel_end','num_start','num_end','start_time','end_time']].iloc[: , :]\n",
    "    #relatived = relatived.iloc[199:500 , :].reset_index()\n",
    "    relatived = relatived.sort_values(by=['case','num_start'], ascending=True)\n",
    "    #print('Relatived: ', len(relatived), 'columns', relatived.columns.tolist())\n",
    "    return relatived\n",
    "\n",
    "def plot_selected_traces(snippet, output_path=None):\n",
    "    #plot_point_transformer('Point transformer: Trace \\''+ str(snippet['case'][0]) + '\\' only', snippet)\n",
    "    traces_selection = snippet['case'].drop_duplicates().tolist()[0:3]\n",
    "    #traces_selection = [unique_trace[1]]\n",
    "    data_selection = snippet.loc[snippet['case'].isin(traces_selection)].reset_index().iloc[:]\n",
    "    if len(data_selection[data_selection['num_start']>0])>0:\n",
    "        point = data_selection[data_selection['num_start']>0].sample(n=1)\n",
    "        #point = data_selection.iloc[ 1 , : ].to_frame().transpose()\n",
    "        figurept = plot_point_transformer('Point transformer: Trace '+ str(traces_selection) + ' only, Allen\\'s point: '+str(point['activity'].values)+' in '+str(point['case'].values),\n",
    "                                          data_selection, size=1, traces=traces_selection, allen_point=point, output_path=output_path)\n",
    "    else: \n",
    "        figurept = plot_point_transformer('Point transformer: Trace '+ str(traces_selection) + ' only.',\n",
    "                                          data_selection, size=1, traces=traces_selection, output_path=output_path)\n",
    "    #print(data_selection[['activity','rel_start','rel_end']])\n",
    "    \n",
    "def plot_all_traces(snippet, output_path=None, draw_skylines=None):\n",
    "    traces_selection = snippet['case'].drop_duplicates().tolist()\n",
    "    #print(point)\n",
    "    if len(snippet[snippet['num_start']>0])>0:\n",
    "        point = snippet[snippet['num_start']>0].sample(n=1)\n",
    "        #point = snippet.iloc[ 1 , : ].to_frame().transpose()\n",
    "        figurept = plot_point_transformer('Point transformer: All activities in all traces. Allen\\'s point: '+str(point['activity'].values)+' in '+str(point['case'].values), snippet, allen_point=point, traces=traces_selection,  size=1 , draw_skylines=draw_skylines, output_path=output_path)\n",
    "        #plot_point_transformer('Point transformer: All activities in all traces', snippet, size=1, allen_point=snippet[(snippet['case']==4)&(snippet['num_start']==75840)])\n",
    "    else:\n",
    "        figurept = plot_point_transformer('Point transformer: All activities in all traces.', snippet, traces=traces_selection,  size=1 , draw_skylines=1, output_path=output_path)\n",
    "        \n",
    "    \n",
    "def plot_average_trace(snippet, output_path = None, draw_skylines=None):\n",
    "    #FIXME: Average End and start are only taking hours:minutes and not days into account\n",
    "    #print(snippet['activity'].drop_duplicates().tolist())\n",
    "    data_selection = get_data_selection_avgtrace(snippet).iloc[:]\n",
    "    traces_selection = data_selection['case'].drop_duplicates().tolist()\n",
    "    if len(data_selection[data_selection['num_start']>0])>0:\n",
    "        point = data_selection[data_selection['num_start']>0].sample(n=1)\n",
    "        #data_selection.iloc[ 1 , : ].to_frame().transpose()\n",
    "        #print(data_selection)\n",
    "        figurept = plot_point_transformer('Point transformer: Average trace from all activities, Allen\\'s point: '+str(point['activity'].values)+' in '+str(point['case'].values), \n",
    "                                          data_selection, traces=traces_selection, size=1, allen_point=point, output_path=output_path, draw_skylines=draw_skylines)\n",
    "        #plot_point_transformer('Point transformer: Average trace from all activities', snippet, allen_point=point)\n",
    "    else:\n",
    "        figurept = plot_point_transformer('Point transformer: Average trace from all activities', \n",
    "                                          data_selection, traces=traces_selection, size=1, output_path=output_path, draw_skylines=draw_skylines)\n",
    "\n",
    "def plot_selected_activities(snippet, output_path = None):\n",
    "    #TODO: Adapt frame dynamically\n",
    "    #TODO: Add start by zero option\n",
    "    unique_act = snippet['activity'].unique().tolist()\n",
    "    #print('There are ', len(unique_act), 'unique activities.')\n",
    "    activity_selection=unique_act[0]\n",
    "    #print(activity_selection)\n",
    "    figurept = plot_point_transformer('Point transformer: Activity \\''+ str(activity_selection) + '\\' only', snippet , activity=activity_selection, size=1, output_path=output_path)\n",
    "    #print(snippet[snippet['activity']==activity_selection])\n",
    "    \n",
    "def plot_duration_selectedtraces(w_duration, output_path=None):\n",
    "    #TODO: Suspect 'meets' line is wrong\n",
    "    traces_selection = w_duration['case'].drop_duplicates().tolist()[0:3]\n",
    "    if len(w_duration[w_duration['num_start']>0])>0:\n",
    "        point = w_duration[w_duration['num_start']>0].sample(n=1)\n",
    "        #point = w_duration.iloc[ 2 , : ].to_frame().transpose()\n",
    "        #print(point)\n",
    "        figurept = plot_point_transformer('Point transformer: Trace '+ str(traces_selection) + ' only, Allen\\'s point: '+str(point['activity'].values)+' in '+str(point['case'].values),\n",
    "                                          w_duration, duration_plot=1, allen_point=point, traces=traces_selection, size=1, output_path=output_path)\n",
    "        #plot_point_transformer('Point transformer: All activities in all traces', snippet, size=1, allen_point=snippet[(snippet['case']==4)&(snippet['num_start']==75840)])\n",
    "    else:\n",
    "         figurept = plot_point_transformer('Point transformer: Trace '+ str(traces_selection),\n",
    "                                          w_duration, duration_plot=1, traces=traces_selection, size=1, output_path=output_path)   \n",
    "\n",
    "#TODO: Draw skylines\n",
    "def plot_duration_alltraces(w_duration, output_path=None): \n",
    "    traces_selection= w_duration['case'].drop_duplicates().tolist()\n",
    "    if len(w_duration[w_duration['num_start']>0])>0:\n",
    "        point = w_duration[w_duration['num_start']>0].sample(n=1)\n",
    "        #point = w_duration.iloc[ 5 , : ].to_frame().transpose()\n",
    "        #print(appended[['case','activity','start_time','end_time']].sort_values(by=['case']))\n",
    "        figurept = plot_point_transformer('Point transformer: All activities in all traces', w_duration, size=1, duration_plot=1, allen_point=point, traces=traces_selection, output_path=output_path)\n",
    "        #plot_point_transformer('Point transformer: All activities in all traces', snippet, size=1, allen_point=snippet[(snippet['case']==4)&(snippet['num_start']==75840)])\n",
    "    else: \n",
    "        figurept = plot_point_transformer('Point transformer: All activities in all traces', w_duration, size=1, duration_plot=1, traces=traces_selection, output_path=output_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_point_transformer_selection(subset, output_path_prefix):\n",
    "    activity = subset['activity'].apply(lambda row: row.split('(',1)[0]).unique().tolist()\n",
    "    #filename_addition = title_from_list(activity)\n",
    "    filename_addition = ''\n",
    "    output_path_prefix += '_'+filename_addition\n",
    "    print('\\nSubset of ', activity, 'has:')\n",
    "    print(len(subset), ' entries')\n",
    "    \n",
    "    unique_act = subset['activity'].unique().tolist()\n",
    "    print(len(unique_act), 'different activities')\n",
    "    #print(unique_act['activity'].tolist(),'\\n')\n",
    "    \n",
    "    unique_trace = subset['case'].unique().tolist()\n",
    "    print(len(unique_trace), ' cases')\n",
    "    #print(unique_trace, '\\n')\n",
    "    \n",
    "    snippet = get_relative_timestamps(subset)\n",
    "    \n",
    "    outputpath_seltr = output_path_prefix+'point_transformer_selectedTraces'+'.png'\n",
    "    #print(outputpath_seltr)\n",
    "    #plot_selected_traces(snippet, output_path=outputpath_seltr)\n",
    "    \n",
    "    output_path_atr = output_path_prefix+'point_transformer_allTraces'+'.png'\n",
    "    #print(output_path_atr)\n",
    "   # plot_all_traces(snippet, output_path=output_path_atr)\n",
    "    \n",
    "    output_path_atr = output_path_prefix+'point_transformer_allTraces_skyline'+'.png'\n",
    "    #print(output_path_atr)\n",
    "    #plot_all_traces(snippet, output_path=output_path_atr, draw_skylines=1)\n",
    "    \n",
    "    output_path_avtr = output_path_prefix+'point_transformer_averageTrace'+'.png'\n",
    "    #print(output_path_avtr)\n",
    "    #plot_average_trace(snippet, output_path=output_path_avtr)\n",
    "    \n",
    "    output_path_avtr = output_path_prefix+'point_transformer_averageTrace_skyline'+'.png'\n",
    "    #print(output_path_avtr)\n",
    "    plot_average_trace(snippet, output_path=output_path_avtr, draw_skylines=1)\n",
    "    \n",
    "    output_path_sa = output_path_prefix+'point_transformer_selectedAct'+'.png'\n",
    "    #print(output_path_sa)\n",
    "    #plot_selected_activities(snippet, output_path=output_path_sa)\n",
    "    \n",
    "    w_duration = snippet.copy()\n",
    "    w_duration['duration'] = w_duration.apply(lambda row: str(get_duration(str(row['start_time']),str(row['end_time']))), axis=1)\n",
    "    w_duration['rel_end']=w_duration['duration']\n",
    "    w_duration['t_duration']= w_duration.apply(lambda row: (get_duration(str(row['start_time']),str(row['end_time'])).total_seconds()), axis=1)\n",
    "    w_duration['num_end']=w_duration['t_duration']\n",
    "    w_duration = w_duration[['case','activity','rel_start','num_start', 'rel_end', 'num_end']]\n",
    "\n",
    "    #print(w_duration.columns)\n",
    "    #print(len(w_duration))\n",
    "    \n",
    "    output_path_st_duration = output_path_prefix+'point_transformer_duration_selectedTraces'+'.png'\n",
    "    #print(output_path_st_duration)\n",
    "    #plot_duration_selectedtraces(w_duration, output_path=output_path_st_duration)\n",
    "    \n",
    "    output_path_duration = output_path_prefix+'point_transformer_duration_allTraces'+'.png'\n",
    "    #print(output_path_duration)\n",
    "    #plot_duration_alltraces(w_duration, output_path=output_path_duration)\n",
    "\n",
    "    return snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404  length of subset\n",
      "\n",
      "Subset of  ['CrawlFeedTask', 'ProxySetupTask', 'DumpTask', 'HAProxySetupTask', 'ResolveFeedIDsTask', 'ConvertFeedTask', 'ScaleScoreFeedTask', 'ConvertDumpTask', 'FetchInputTask', 'UpdateCrawlStartTask', 'SplitCrawlInputTask'] has:\n",
      "404  entries\n",
      "31 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_0_31act_404pts_point_transformer_averageTrace_skyline.png\n",
      "390  length of subset\n",
      "\n",
      "Subset of  ['ConvertDumpTask', 'UpdateCrawlStartTask', 'SplitCrawlInputTask', 'DumpTask', 'PartitionCrawlInputTask', 'StatsCrawlInputTask', 'CrawlTask', 'ExtractTask', 'CrawlChunkTask', 'ExtractChunkTask'] has:\n",
      "390  entries\n",
      "30 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_21_51act_390pts_point_transformer_averageTrace_skyline.png\n",
      "390  length of subset\n",
      "\n",
      "Subset of  ['ExtractTask', 'CrawlChunkTask', 'ExtractChunkTask', 'DumpTask', 'ConvertDumpTask'] has:\n",
      "390  entries\n",
      "30 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_41_71act_390pts_point_transformer_averageTrace_skyline.png\n",
      "390  length of subset\n",
      "\n",
      "Subset of  ['ConvertDumpTask', 'DumpTask'] has:\n",
      "390  entries\n",
      "30 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_61_91act_390pts_point_transformer_averageTrace_skyline.png\n",
      "390  length of subset\n",
      "\n",
      "Subset of  ['ConvertDumpTask', 'DumpTask', 'CleanupStarwoodDataTask'] has:\n",
      "390  entries\n",
      "30 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_81_111act_390pts_point_transformer_averageTrace_skyline.png\n",
      "390  length of subset\n",
      "\n",
      "Subset of  ['ConvertDumpTask', 'DumpTask', 'CleanupStarwoodDataTask', 'ClusterScoreTask', 'ConvertDumpXmlifyTask', 'PartitionConvertDumpTask', 'PartitionConvertDumpXmlifyTask', 'UpdateMgmtResponseTask'] has:\n",
      "390  entries\n",
      "30 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_87_117act_390pts_point_transformer_averageTrace_skyline.png\n",
      "3341  length of subset\n",
      "\n",
      "Subset of  ['CrawlTask', 'ConvertDumpTask', 'DumpTask', 'CleanupStarwoodDataTask', 'ClusterScoreTask', 'ConvertDumpXmlifyTask', 'PartitionConvertDumpTask', 'PartitionConvertDumpXmlifyTask', 'UpdateMgmtResponseTask'] has:\n",
      "3341  entries\n",
      "18 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_101_119act_3341pts_point_transformer_averageTrace_skyline.png\n",
      "6253  length of subset\n",
      "\n",
      "Subset of  ['CrawlTask', 'ExtractTask', 'ConvertDumpXmlifyTask'] has:\n",
      "6253  entries\n",
      "3 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_117_120act_6253pts_point_transformer_averageTrace_skyline.png\n",
      "3159  length of subset\n",
      "\n",
      "Subset of  ['ExtractTask', 'ConvertDumpXmlifyTask', 'DBDumpTask', 'PrepTask'] has:\n",
      "3159  entries\n",
      "4 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_118_122act_3159pts_point_transformer_averageTrace_skyline.png\n",
      "1792  length of subset\n",
      "\n",
      "Subset of  ['CrawlTask', 'DBDumpTask', 'PrepTask', 'CrawlChunkTask', 'StatsCrawlTask', 'ExtractChunkTask'] has:\n",
      "1792  entries\n",
      "6 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_120_126act_1792pts_point_transformer_averageTrace_skyline.png\n",
      "2139  length of subset\n",
      "\n",
      "Subset of  ['ExtractTask', 'CrawlChunkTask', 'StatsCrawlTask', 'ExtractChunkTask', 'QCValidateExtractTask', 'QCValidateExtractXmlifyTask', 'QCCountTask', 'QCUpdateTask', 'QCValidateTask', 'SourceScoreTask', 'QCValidateJsonifyTask', 'ReviewDedupTask', 'CrawlPrivateDataTask', 'QCUpdateCrawlDateTask', 'QualityCheckTask', 'PrevChunkNewReviewsOriginChangeTask', 'ReportTask', 'ChunkReviewTimelineTask', 'ConvertDedupTask', 'NewPublicDataTask', 'NewPrivateDataTask', 'GeneratePIITask', 'StripPIITask', 'MergeNewData', 'FilterOutResponsesTask', 'DetectLanguageTask', 'UpdateReviewPropsTask', 'TokenizeTask', 'SemaTask', 'ConsolidatedAlertsTask', 'UpdateDatastoreTask', 'ResyncReviewPropertiesTask'] has:\n",
      "2139  entries\n",
      "33 different activities\n",
      "13  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_123_156act_2139pts_point_transformer_averageTrace_skyline.png\n",
      "372  length of subset\n",
      "\n",
      "Subset of  ['PrevChunkNewReviewsOriginChangeTask', 'ReportTask', 'ChunkReviewTimelineTask', 'ConvertDedupTask', 'NewPublicDataTask', 'NewPrivateDataTask', 'GeneratePIITask', 'StripPIITask', 'MergeNewData', 'FilterOutResponsesTask', 'DetectLanguageTask', 'UpdateReviewPropsTask', 'TokenizeTask', 'SemaTask', 'ConsolidatedAlertsTask', 'UpdateDatastoreTask', 'ResyncReviewPropertiesTask', 'UpdateDBTask', 'CrawlTask', 'ExtractTask', 'CrawlChunkTask', 'CleanupStarwoodDataTask', 'ExtractChunkTask', 'UserDemographicsFullUpdateTask', 'UserDemographicsIncUpdateCQLTask', 'UserDemographicsIncUpdateApplyTask', 'AlertsTask', 'GroupTask', 'GroupPreDumpTask'] has:\n",
      "372  entries\n",
      "31 different activities\n",
      "12  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_139_170act_372pts_point_transformer_averageTrace_skyline.png\n",
      "1789  length of subset\n",
      "\n",
      "Subset of  ['PrevChunkNewReviewsOriginChangeTask', 'ReportTask', 'NewPublicDataTask', 'NewPrivateDataTask', 'GeneratePIITask', 'StripPIITask', 'MergeNewData', 'FilterOutResponsesTask', 'DetectLanguageTask', 'UpdateReviewPropsTask', 'TokenizeTask', 'SemaTask', 'ConsolidatedAlertsTask', 'UpdateDatastoreTask', 'ResyncReviewPropertiesTask', 'CrawlTask', 'UpdateDBTask', 'ExtractTask', 'CrawlChunkTask', 'CleanupStarwoodDataTask', 'ExtractChunkTask', 'UserDemographicsFullUpdateTask', 'UserDemographicsIncUpdateCQLTask', 'UserDemographicsIncUpdateApplyTask', 'AlertsTask', 'GroupTask', 'GroupPreDumpTask'] has:\n",
      "1789  entries\n",
      "30 different activities\n",
      "12  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_141_171act_1789pts_point_transformer_averageTrace_skyline.png\n",
      "1527  length of subset\n",
      "\n",
      "Subset of  ['CrawlTask', 'GroupTask', 'CrawlChunkTask', 'StatsCrawlTask', 'ExtractChunkTask', 'QCValidateExtractTask', 'QCValidateExtractXmlifyTask', 'QCCountTask'] has:\n",
      "1527  entries\n",
      "8 different activities\n",
      "12  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_170_178act_1527pts_point_transformer_averageTrace_skyline.png\n",
      "1923  length of subset\n",
      "\n",
      "Subset of  ['ExtractTask', 'QCValidateExtractTask', 'QCValidateExtractXmlifyTask', 'QCCountTask', 'GroupTask', 'CrawlChunkTask', 'StatsCrawlTask', 'HAProxyTeardownTask', 'ProxyTeardownTask', 'ExtractChunkTask', 'QCUpdateTask', 'QCValidateTask', 'SourceScoreTask', 'QCValidateJsonifyTask', 'QCUpdateCrawlDateTask', 'ReviewDedupTask', 'CrawlPrivateDataTask', 'QualityCheckTask', 'ReportTask', 'PrevChunkNewReviewsOriginChangeTask', 'ChunkReviewTimelineTask', 'ConvertDedupTask', 'NewPublicDataTask', 'NewPrivateDataTask', 'GeneratePIITask', 'StripPIITask', 'MergeNewData', 'FilterOutResponsesTask', 'DetectLanguageTask', 'UpdateReviewPropsTask'] has:\n",
      "1923  entries\n",
      "41 different activities\n",
      "12  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_175_216act_1923pts_point_transformer_averageTrace_skyline.png\n",
      "485  length of subset\n",
      "\n",
      "Subset of  ['ReportTask', 'ChunkReviewTimelineTask', 'ReviewDedupTask', 'NewPublicDataTask', 'NewPrivateDataTask', 'GeneratePIITask', 'StripPIITask', 'MergeNewData', 'FilterOutResponsesTask', 'DetectLanguageTask', 'UpdateReviewPropsTask', 'TokenizeTask', 'SemaTask', 'ConsolidatedAlertsTask', 'UpdateDatastoreTask', 'ResyncReviewPropertiesTask', 'QualityCheckTask', 'UpdateDBTask', 'UserDemographicsFullUpdateTask', 'UserDemographicsIncUpdateCQLTask', 'UserDemographicsIncUpdateApplyTask', 'GroupTask', 'AlertsTask', 'ChunkTask', 'ConvertDedupTask', 'UpdateTimelineTask'] has:\n",
      "485  entries\n",
      "40 different activities\n",
      "12  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_206_246act_485pts_point_transformer_averageTrace_skyline.png\n",
      "449  length of subset\n",
      "\n",
      "Subset of  ['ReviewDedupTask', 'TokenizeTask', 'SemaTask', 'ConsolidatedAlertsTask', 'UpdateDatastoreTask', 'ResyncReviewPropertiesTask', 'QualityCheckTask', 'UpdateDBTask', 'UserDemographicsFullUpdateTask', 'UserDemographicsIncUpdateCQLTask', 'UserDemographicsIncUpdateApplyTask', 'GroupTask', 'AlertsTask', 'ChunkTask', 'ReportTask', 'ChunkReviewTimelineTask', 'ConvertDedupTask', 'NewPublicDataTask', 'NewPrivateDataTask', 'UpdateTimelineTask', 'GeneratePIITask', 'StripPIITask', 'MergeNewData', 'FilterOutResponsesTask', 'UpdateReviewPropsTask', 'DetectLanguageTask', 'RootTask'] has:\n",
      "449  entries\n",
      "37 different activities\n",
      "12  cases\n",
      "Saving in  /usr/local/trustyou/home/andream/nfs/processmining/minilogs/daily/graphs/avgActSlideSplit/param_catts_daily.2019-09-All_216_256act_449pts_point_transformer_averageTrace_skyline.png\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#index_used=[[0,31],[21,51],[41,71],[61,91],[81,111],[87,117],\n",
    "#            [101,119],[117,120],[118,122],[120,126],[123,156],\n",
    "#            [139,170],[141,171],[170,178],[175,216],[206,246],[216,256]]\n",
    "\n",
    "for indexes in index_used:\n",
    "    #subset = appended[appended['activity'].str.startswith('ExtractTask') & appended['case']]\n",
    "    subset = appended[appended['activity'].isin(average_trace_activities[indexes[0]:indexes[1]])]\n",
    "    #subset = appended[appended['activity'].isin(average_trace_activities[indexes[0]:indexes[1]]) & ~appended['activity'].str.startswith('ExtractTask(crawler=creepy-crawly)(chunk')]\n",
    "\n",
    "    output_prefix = LUIGI_LOG_PATH+'graphs/avgActSlideSplit/'+filename+'All_'+str(indexes[0])+'_'+str(indexes[1])+'act_'+str(len(subset))+'pts'\n",
    "    #print(output_prefix)\n",
    "\n",
    "    print(len(subset),' length of subset')\n",
    "    #print(average_trace_activities[indexes[0]:indexes[1]])\n",
    "    #subset.head()\n",
    "    #subset= appended.copy()\n",
    "    #subset = appended[appended['case']=='daily.2019-09-01_09-29-01.csv']\n",
    "    #snippet = plot_point_transformer_selection(subset, output_prefix)\n",
    "    #snippet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_trace_activities = get_data_selection_avgtrace(get_relative_timestamps(appended)).sort_values(by=['num_start'])['activity'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15711\n",
      "Index(['case', 'activity', 'rel_start', 'rel_end', 'num_start', 'num_end',\n",
      "       'start_time', 'end_time'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CrawlTask(crawler=creepy-crawly)(chunk=01)',\n",
       " 'ExtractTask(crawler=creepy-crawly)(chunk=01)',\n",
       " 'ExtractTask(crawler=creepy-crawly)(chunk=03)',\n",
       " 'CrawlTask(crawler=creepy-crawly)(chunk=03)',\n",
       " 'ExtractTask(crawler=creepy-crawly)(chunk=02)',\n",
       " 'CrawlTask(crawler=creepy-crawly)(chunk=02)',\n",
       " 'GroupTask(deploy=False)(chunk=02)(datacenter=eu)(inc=True)(datastore=mongo)',\n",
       " 'ResyncReviewPropertiesTask(chunk=03)',\n",
       " 'GroupTask(deploy=False)(chunk=01)(datacenter=eu)(inc=False)(datastore=mongo)',\n",
       " 'ProxySetupTask',\n",
       " 'DumpTask(target_filename=review_2yold)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2014_1)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2014_2)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2014_3)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2015_2)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2014_4)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2015_1)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2013_3)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2015_3)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2015_4)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2016_1)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2016_2)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2016_3)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2016_4)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2013_4)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2012_3)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2013_2)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2013_1)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=None)(chunk=prep)(sql_filename=consolidated_email)',\n",
       " 'DumpTask(target_filename=None)(chunk=prep)(sql_filename=daily_urls)',\n",
       " 'DumpTask(target_filename=None)(chunk=prep)(sql_filename=mgmt_response)',\n",
       " 'DumpTask(target_filename=None)(chunk=prep)(sql_filename=other_urls)',\n",
       " 'DumpTask(target_filename=None)(chunk=prep)(sql_filename=regular_urls)',\n",
       " 'DumpTask(target_filename=None)(chunk=prep)(sql_filename=reprocess_urls)',\n",
       " 'DumpTask(target_filename=None)(chunk=prep)(sql_filename=source)',\n",
       " 'DumpTask(target_filename=review_2011_1)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2011_2)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2011_3)(chunk=prep)(sql_filename=review)',\n",
       " 'QCValidateJsonifyTask(chunk=01)',\n",
       " 'DumpTask(target_filename=review_2012_1)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2012_2)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2017_2)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2012_4)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2017_1)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2018_2)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2017_3)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2017_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ScaleScoreFeedTask(what=source)',\n",
       " 'HAProxySetupTask',\n",
       " 'ScaleScoreFeedTask(what=review)',\n",
       " 'ResolveFeedIDsTask',\n",
       " 'PartitionConvertDumpTask',\n",
       " 'PartitionConvertDumpXmlifyTask',\n",
       " 'PartitionCrawlInputTask(crawler=creepy-crawly)',\n",
       " 'PartitionCrawlInputTask(crawler=ty-superman)',\n",
       " 'PrepTask',\n",
       " 'QCCountTask(chunk=01)',\n",
       " 'QCValidateTask(what=source)(chunk=01)',\n",
       " 'QCUpdateTask(chunk=01)',\n",
       " 'QCValidateExtractTask(chunk=01)',\n",
       " 'QCValidateTask(what=review)(chunk=01)',\n",
       " 'QCValidateExtractXmlifyTask(chunk=01)',\n",
       " 'SplitCrawlInputTask',\n",
       " 'StatsCrawlInputTask(chunk=01)',\n",
       " 'StatsCrawlInputTask(chunk=02)',\n",
       " 'DumpTask(target_filename=review_2019_3)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2018_1)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=None)(chunk=01)(sql_filename=consolidated_email)',\n",
       " 'DumpTask(target_filename=review_2018_3)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2018_4)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2019_1)(chunk=prep)(sql_filename=review)',\n",
       " 'DumpTask(target_filename=review_2019_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ExtractChunkTask(crawler=creepy-crawly)(chunk=01)',\n",
       " 'StatsCrawlInputTask(chunk=03)',\n",
       " 'ExtractChunkTask(crawler=ty-superman)(chunk=01)',\n",
       " 'ExtractChunkTask(crawler=ty-superman)(chunk=03)',\n",
       " 'ExtractTask(crawler=ty-superman)(chunk=01)',\n",
       " 'ExtractTask(crawler=ty-superman)(chunk=03)',\n",
       " 'FetchInputTask',\n",
       " 'StatsCrawlTask(chunk=01)',\n",
       " 'DumpTask(target_filename=None)(chunk=02)(sql_filename=consolidated_email)',\n",
       " 'DumpTask(target_filename=review_2011_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2012_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2017_1)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2016_3)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2016_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2016_1)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2015_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2015_3)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2015_1)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2014_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2014_3)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2014_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2014_1)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2013_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2013_3)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2013_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2013_1)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2012_3)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2012_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2012_1)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2011_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2011_3)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2011_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2011_1)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=None)(chunk=prep)(sql_filename=source)',\n",
       " 'ConvertDumpTask(target_filename=None)(chunk=prep)(sql_filename=mgmt_response)',\n",
       " 'CleanupStarwoodDataTask(chunk=01)',\n",
       " 'ConvertDumpTask(target_filename=None)(chunk=prep)(sql_filename=consolidated_email)',\n",
       " 'ConvertDumpTask(target_filename=None)(chunk=02)(sql_filename=consolidated_email)',\n",
       " 'ConvertDumpTask(target_filename=None)(chunk=01)(sql_filename=consolidated_email)',\n",
       " 'CleanupStarwoodDataTask(chunk=03)',\n",
       " 'ClusterScoreTask',\n",
       " 'ConvertDumpTask(target_filename=review_2016_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2015_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2017_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpXmlifyTask(what=mgmt_response)',\n",
       " 'DBDumpTask',\n",
       " 'CrawlTask(crawler=ty-superman)(chunk=03)',\n",
       " 'UpdateCrawlStartTask',\n",
       " 'CrawlTask(crawler=ty-superman)(chunk=01)',\n",
       " 'CrawlFeedTask',\n",
       " 'CrawlChunkTask(crawler=ty-superman)(chunk=03)',\n",
       " 'CrawlChunkTask(crawler=ty-superman)(chunk=01)',\n",
       " 'ConvertDumpTask(target_filename=review_2017_3)(chunk=prep)(sql_filename=review)',\n",
       " 'CrawlChunkTask(crawler=creepy-crawly)(chunk=01)',\n",
       " 'ConvertFeedTask(what=source)',\n",
       " 'ConvertFeedTask(what=review)',\n",
       " 'ConvertDumpXmlifyTask(what=source)',\n",
       " 'UpdateMgmtResponseTask',\n",
       " 'ConvertDumpTask(target_filename=review_2017_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2019_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2018_2)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2018_4)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2018_1)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2019_1)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2018_3)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2019_3)(chunk=prep)(sql_filename=review)',\n",
       " 'ConvertDumpTask(target_filename=review_2yold)(chunk=prep)(sql_filename=review)',\n",
       " 'UserDemographicsIncUpdateCQLTask(chunk=02)',\n",
       " 'QCValidateJsonifyTask(chunk=03)',\n",
       " 'UserDemographicsFullUpdateTask(chunk=01)',\n",
       " 'UpdateTimelineTask',\n",
       " 'ReportTask(chunk=03)',\n",
       " 'UserDemographicsFullUpdateTask(chunk=02)',\n",
       " 'ReportTask(chunk=02)',\n",
       " 'ReportTask(chunk=01)',\n",
       " 'QCValidateTask(what=source)(chunk=02)',\n",
       " 'UserDemographicsFullUpdateTask(chunk=03)',\n",
       " 'UserDemographicsIncUpdateCQLTask(chunk=01)',\n",
       " 'QualityCheckTask(chunk=03)',\n",
       " 'QCValidateTask(what=source)(chunk=03)',\n",
       " 'UserDemographicsIncUpdateApplyTask(chunk=03)',\n",
       " 'ResyncReviewPropertiesTask(chunk=01)',\n",
       " 'QCValidateTask(what=review)(chunk=02)',\n",
       " 'QCValidateTask(what=review)(chunk=03)',\n",
       " 'QualityCheckTask(chunk=02)',\n",
       " 'QualityCheckTask(chunk=01)',\n",
       " 'UserDemographicsIncUpdateApplyTask(chunk=02)',\n",
       " 'UserDemographicsIncUpdateApplyTask(chunk=01)',\n",
       " 'UpdateReviewPropsTask(chunk=03)',\n",
       " 'TokenizeTask(chunk=01)',\n",
       " 'ResyncReviewPropertiesTask(chunk=02)',\n",
       " 'UpdateReviewPropsTask(chunk=02)',\n",
       " 'TokenizeTask(chunk=03)',\n",
       " 'UpdateDBTask(chunk=01)',\n",
       " 'UpdateDBTask(chunk=02)',\n",
       " 'UpdateDBTask(chunk=03)',\n",
       " 'UpdateDatastoreTask(chunk=01)',\n",
       " 'UpdateDatastoreTask(chunk=02)',\n",
       " 'StripPIITask(chunk=03)',\n",
       " 'StripPIITask(chunk=02)',\n",
       " 'StripPIITask(chunk=01)',\n",
       " 'StatsCrawlTask(chunk=03)',\n",
       " 'TokenizeTask(chunk=02)',\n",
       " 'StatsCrawlTask(chunk=02)',\n",
       " 'SourceScoreTask(chunk=03)',\n",
       " 'SourceScoreTask(chunk=02)',\n",
       " 'SourceScoreTask(chunk=01)',\n",
       " 'SemaTask(chunk=03)',\n",
       " 'SemaTask(chunk=02)',\n",
       " 'SemaTask(chunk=01)',\n",
       " 'UpdateReviewPropsTask(chunk=01)',\n",
       " 'RootTask',\n",
       " 'ReviewDedupTask(chunk=03)',\n",
       " 'ReviewDedupTask(chunk=02)',\n",
       " 'ReviewDedupTask(chunk=01)',\n",
       " 'QCValidateJsonifyTask(chunk=02)',\n",
       " 'AlertsTask(chunk=01)',\n",
       " 'QCValidateExtractXmlifyTask(chunk=03)',\n",
       " 'QCValidateExtractXmlifyTask(chunk=02)',\n",
       " 'FilterOutResponsesTask(chunk=02)',\n",
       " 'FilterOutResponsesTask(chunk=01)',\n",
       " 'ExtractTask(crawler=ty-superman)(chunk=02)',\n",
       " 'ExtractChunkTask(crawler=ty-superman)(chunk=02)',\n",
       " 'ExtractChunkTask(crawler=creepy-crawly)(chunk=03)',\n",
       " 'ExtractChunkTask(crawler=creepy-crawly)(chunk=02)',\n",
       " 'AlertsTask(chunk=02)',\n",
       " 'DetectLanguageTask(chunk=03)',\n",
       " 'DetectLanguageTask(chunk=02)',\n",
       " 'DetectLanguageTask(chunk=01)',\n",
       " 'CrawlTask(crawler=ty-superman)(chunk=02)',\n",
       " 'CrawlPrivateDataTask(chunk=03)',\n",
       " 'CrawlPrivateDataTask(chunk=02)',\n",
       " 'CrawlPrivateDataTask(chunk=01)',\n",
       " 'CrawlChunkTask(crawler=ty-superman)(chunk=02)',\n",
       " 'CrawlChunkTask(crawler=creepy-crawly)(chunk=03)',\n",
       " 'CrawlChunkTask(crawler=creepy-crawly)(chunk=02)',\n",
       " 'ConvertDedupTask(chunk=03)',\n",
       " 'ConvertDedupTask(chunk=02)',\n",
       " 'ConvertDedupTask(chunk=01)',\n",
       " 'ConsolidatedAlertsTask(chunk=02)',\n",
       " 'ConsolidatedAlertsTask(chunk=01)',\n",
       " 'CleanupStarwoodDataTask(chunk=02)',\n",
       " 'ChunkTask(chunk=03)',\n",
       " 'ChunkTask(chunk=02)',\n",
       " 'ChunkTask(chunk=01)',\n",
       " 'ChunkReviewTimelineTask(chunk=03)',\n",
       " 'ChunkReviewTimelineTask(chunk=02)',\n",
       " 'ChunkReviewTimelineTask(chunk=01)',\n",
       " 'FilterOutResponsesTask(chunk=03)',\n",
       " 'GeneratePIITask(chunk=01)',\n",
       " 'GeneratePIITask(chunk=02)',\n",
       " 'NewPublicDataTask(chunk=02)',\n",
       " 'QCValidateExtractTask(chunk=03)',\n",
       " 'QCValidateExtractTask(chunk=02)',\n",
       " 'QCUpdateTask(chunk=03)',\n",
       " 'QCUpdateTask(chunk=02)',\n",
       " 'QCUpdateCrawlDateTask(chunk=03)',\n",
       " 'QCUpdateCrawlDateTask(chunk=02)',\n",
       " 'QCUpdateCrawlDateTask(chunk=01)',\n",
       " 'QCCountTask(chunk=03)',\n",
       " 'QCCountTask(chunk=02)',\n",
       " 'ProxyTeardownTask',\n",
       " 'PrevChunkNewReviewsOriginChangeTask(chunk=3)',\n",
       " 'PrevChunkNewReviewsOriginChangeTask(chunk=2)',\n",
       " 'NewPublicDataTask(chunk=03)',\n",
       " 'NewPublicDataTask(chunk=01)',\n",
       " 'GeneratePIITask(chunk=03)',\n",
       " 'NewPrivateDataTask(chunk=03)',\n",
       " 'NewPrivateDataTask(chunk=02)',\n",
       " 'NewPrivateDataTask(chunk=01)',\n",
       " 'MergeNewData(chunk=03)',\n",
       " 'MergeNewData(chunk=02)',\n",
       " 'MergeNewData(chunk=01)',\n",
       " 'HAProxyTeardownTask',\n",
       " 'GroupTask(deploy=True)(chunk=02)(datacenter=eu)(inc=True)(datastore=tch)',\n",
       " 'GroupTask(deploy=True)(chunk=01)(datacenter=us)(inc=False)(datastore=tch)',\n",
       " 'GroupTask(deploy=True)(chunk=01)(datacenter=eu)(inc=True)(datastore=tch)',\n",
       " 'GroupTask(deploy=True)(chunk=01)(datacenter=eu)(inc=False)(datastore=tch)',\n",
       " 'GroupTask(deploy=False)(chunk=01)(datacenter=eu)(inc=True)(datastore=mongo)',\n",
       " 'GroupPreDumpTask',\n",
       " 'UserDemographicsIncUpdateCQLTask(chunk=03)']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_prefix = LUIGI_LOG_PATH+'graphs/'+filename+'_0_500'\n",
    "print(len(appended))\n",
    "subset = appended.iloc[0:501]\n",
    "#snippet = plot_point_transformer_selection(subset, output_prefix)\n",
    "\n",
    "\n",
    "i=0\n",
    "relatived = get_relative_timestamps(appended).sort_values(by='num_start')\n",
    "print(relatived.columns)\n",
    "\n",
    "#while i<len(relatived):\n",
    "#    output_prefix = LUIGI_LOG_PATH+'graphs/'+filename+'All_'+str(i)+'_'+str(min(i+500,len(relatived)))\n",
    "#    subset = relatived.iloc[i:i+501]\n",
    "#    #print(output_prefix)\n",
    "#    snippet = plot_point_transformer_selection(subset, output_prefix)\n",
    "#    i+=501\n",
    "    \n",
    "relatived.head(5)\n",
    "#for i, short_activity in enumerate(unique_short_activities[34:]):\n",
    "#    print(i+34,'/',len(unique_short_activities))\n",
    "#    subset = appended[appended['activity'].str.startswith(short_activity)]\n",
    "#    plot_point_transformer_selection(subset, output_prefix)\n",
    "\n",
    "    #print(short_activity, 'has: ', len(subset), ' entries.')\n",
    "    \n",
    "relatived.groupby(['activity']).size().reset_index(name='counts').sort_values(by=['counts'], ascending=False)['activity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlertsTask']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_short_activities[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
